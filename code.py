# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gxadOmsYK5gaR8fsysdPw7yAbBifduHL
"""

from google.colab import drive
drive.mount('/content/drive')

pip install ucimlrepo

pip install pandas numpy scikit-learn xgboost imbalanced-learn

import pandas as pd
import numpy as np
import seaborn as sn
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV

## ------- Models --------
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn import tree
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor


from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier

##---- Deep Learning Models ----
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

## ---------- Multi Class Classidier -----------
from sklearn.multiclass import OneVsRestClassifier # one to rest
from sklearn.multiclass import OneVsOneClassifier  # one to one

## ----------- Scores ----------------
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix,classification_report

## -------- Ensemble -------
from sklearn.ensemble import BaggingClassifier

from ucimlrepo import fetch_ucirepo

# fetch dataset
cervical_cancer_behavior_risk = fetch_ucirepo(id=537)

# data (as pandas dataframes)
X = cervical_cancer_behavior_risk.data.features
y = cervical_cancer_behavior_risk.data.targets

# metadata
df=cervical_cancer_behavior_risk.metadata
print(df)

X.replace('?', np.nan, inplace=True)
X = X.apply(pd.to_numeric, errors='coerce')

y.replace('?', np.nan, inplace=True)
y = y.apply(pd.to_numeric, errors='coerce')

import pandas as pd

data = pd.DataFrame(X)
data['target'] = y
data

from sklearn.utils import shuffle
data = shuffle(data)
data

data.info()

correlation_matrix = data.corr()

# Create a heatmap to visualize the correlation matrix
plt.figure(figsize=(20, 15))
sn.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Breast Cancer Wisconsin Prognostic Features')
plt.show()

# Drop unnecessary columns first
columns_to_drop = [
    'behavior_personalHygiene', 'empowerment_knowledge', 'socialSupport_instrumental',
    'intention_aggregation', 'intention_commitment', 'norm_significantPerson',
    'perception_vulnerability', 'behavior_eating', 'socialSupport_appreciation'
]

# Filter out columns that are not in the DataFrame
columns_to_drop_existing = [col for col in columns_to_drop if col in data.columns]

data.drop(columns=columns_to_drop_existing, axis=1, inplace=True)

# Now split features and target
X = data.drop('target', axis=1)
y = data['target']

import matplotlib.pyplot as plt

# Count the values in your imputed target column
class_counts = y.value_counts()
labels = class_counts.index.map({0: 'Negative', 1: 'Positive'})
colors = ['blue', 'lightgreen']

# Create pie chart
plt.figure(figsize=(6, 6))
plt.pie(class_counts, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)
plt.title('Biopsy Class Distribution')
plt.axis('equal')  # Equal aspect ratio ensures pie is a circle
plt.show()

from imblearn.over_sampling import SMOTE
from collections import Counter

print("Before SMOTE:", Counter(y))

smote = SMOTE(sampling_strategy={1: 40}, random_state=42)
x_r, y_r = smote.fit_resample(X, y)

print("After SMOTE:", Counter(y_r))

from scipy.stats import f_oneway

f_oneway(y , y_r)

x = x_r
y = y_r

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    x, y, test_size=0.10, stratify=y, random_state=42
)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0],
}
# Step 8: Hyperparameter tuning with GridSearchCV
grid_search = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    scoring='f1',
    cv=3,
    n_jobs=-1,
    verbose=1
)
grid_search.fit(X_train_scaled, y_train)

# Step 9: Evaluate best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_scaled)

print("\nâœ… Best Parameters:", grid_search.best_params_)
print("\nðŸ“Š Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nðŸ“ˆ Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix

# Step 1: Define the SVM model
svm = SVC()

# Step 2: Define the parameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],            # Regularization strength
    'kernel': ['linear', 'rbf'],       # Try both linear and RBF kernels
    'gamma': ['scale', 'auto', 0.01, 0.001]  # Kernel coefficient (for 'rbf')
}

# Step 3: Set up GridSearchCV
grid_search = GridSearchCV(
    estimator=svm,
    param_grid=param_grid,
    scoring='f1',    # Or 'accuracy', 'recall', 'f1'
    cv=5,
    verbose=1,
    n_jobs=-1
)

# Step 4: Fit on training data (use resampled data if using SMOTE)
grid_search.fit(X_train_scaled, y_train)

# Step 5: Evaluate
best_svm = grid_search.best_estimator_
y_pred = best_svm.predict(X_test_scaled)

print("âœ… Best Parameters:", grid_search.best_params_)
print("âœ… Best Score (CV):", grid_search.best_score_)
print("\nðŸ“Š Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nðŸ“ˆ Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define the parameter distribution
param_dist = {
    'n_estimators': randint(100, 300),          # number of trees
    'max_depth': [4, 6, 8, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2'],
    'bootstrap': [True, False],
    'class_weight': ['balanced']
}

# Initialize model
rf = RandomForestClassifier(random_state=42)

# Setup randomized search
random_search_rf = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_dist,
    n_iter=20,  # Try 20 random combinations
    cv=3,
    scoring='f1',  # or 'accuracy', 'f1', etc.
    random_state=42,
    n_jobs=-1,
    verbose=2
)

# Fit on training data (resampled and scaled)
random_search_rf.fit(X_train_scaled, y_train)

# Best model
best_rf = random_search_rf.best_estimator_

# Show results
print("\nâœ… Best Parameters:", random_search_rf.best_params_)
print("ðŸ“ˆ Best Score (CV):", random_search_rf.best_score_)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import loguniform

# Define parameter distribution
param_dist = {
    'C': loguniform(0.001, 100),              # Regularization strength
    'penalty': ['l1', 'l2'],                  # Type of regularization
    'solver': ['liblinear', 'saga'],          # Solvers that support both l1 and l2
    'class_weight': ['balanced'],             # To handle class imbalance
    'max_iter': [100, 200, 300, 500]           # Number of iterations
}

# Initialize logistic regression
lg = LogisticRegression(random_state=42)

# Set up RandomizedSearchCV
random_search_lg = RandomizedSearchCV(
    estimator=lg,
    param_distributions=param_dist,
    n_iter=20,                    # Try 20 random combinations
    cv=3,
    scoring='f1',          # or 'accuracy', 'f1', etc.
    random_state=42,
    n_jobs=-1,
    verbose=2
)

# Fit the search on scaled & resampled data
random_search_lg.fit(X_train_scaled, y_train)

# Best model
best_lg = random_search_lg.best_estimator_

# Print best parameters
print("\nâœ… Best Parameters:", random_search_lg.best_params_)
print("ðŸ“ˆ Best Score (CV):", random_search_lg.best_score_)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import RandomizedSearchCV

# Define parameter distributions
param_dist = {
    'n_neighbors': [3, 5, 7, 9, 11, 13, 15],             # Number of neighbors
    'weights': ['uniform', 'distance'],                 # Weight function
    'metric': ['euclidean', 'manhattan', 'minkowski'],  # Distance metrics
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']  # Search algorithm
}

# Initialize model
knn = KNeighborsClassifier()

# Set up RandomizedSearchCV
random_search_knn = RandomizedSearchCV(
    estimator=knn,
    param_distributions=param_dist,
    n_iter=20,                  # Try 20 random combinations
    cv=3,
  scoring='f1',        # You can use 'accuracy', 'f1', etc.
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# Fit the model (use resampled and scaled training data)
random_search_knn.fit(X_train_scaled, y_train)

# Get the best model
best_knn = random_search_knn.best_estimator_

# Print results
print("\nâœ… Best Parameters:", random_search_knn.best_params_)
print("ðŸ“ˆ Best Score (CV):", random_search_knn.best_score_)

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define parameter distributions
param_dist = {
    'criterion': ['gini', 'entropy'],              # Splitting criterion
    'max_depth': [3, 5, 7, 10, 15, 20, None],       # Maximum depth of tree
    'min_samples_split': randint(2, 10),           # Minimum samples to split a node
    'min_samples_leaf': randint(1, 10),            # Minimum samples in a leaf
    'max_features': ['auto', 'sqrt', 'log2', None],# Max features to consider for best split
    'class_weight': ['balanced']                   # To handle class imbalance
}

# Initialize model
dt = DecisionTreeClassifier(random_state=42)

# Setup RandomizedSearchCV
random_search_dt = RandomizedSearchCV(
    estimator=dt,
    param_distributions=param_dist,
    n_iter=20,                   # Try 20 random combinations
    cv=3,
   scoring='f1',         # You can also use 'f1', 'accuracy', etc.
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# Fit the model (on resampled, scaled training data)
random_search_dt.fit(X_train_scaled, y_train)

# Best model
best_dt = random_search_dt.best_estimator_

# Print best parameters and score
print("\nâœ… Best Parameters:", random_search_dt.best_params_)
print("ðŸ“ˆ Best Score (CV):", random_search_dt.best_score_)

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from scipy.stats import randint, uniform

# Define parameter distribution
param_dist = {
    'n_estimators': randint(50, 300),
    'learning_rate': uniform(0.01, 1.0),
    'estimator': [
        DecisionTreeClassifier(max_depth=d, random_state=42) for d in [1, 2, 3, 4, 5]
    ],
    # 'algorithm': ['SAMME', 'SAMME.R'],  # Optional for multi-class
}

# Initialize AdaBoost
ada = AdaBoostClassifier(random_state=42)

# RandomizedSearchCV
random_search_ada = RandomizedSearchCV(
    estimator=ada,
    param_distributions=param_dist,
    n_iter=20,
    cv=3,
   scoring='f1',  # or 'f1', 'accuracy'
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# Fit the model
random_search_ada.fit(X_train, y_train)

# Best model
best_ada = random_search_ada.best_estimator_

# Print results
print("\nâœ… Best Parameters:", random_search_ada.best_params_)
print("ðŸ“ˆ Best Score (CV):", random_search_ada.best_score_)

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# Define parameter distributions
param_dist = {
    'n_estimators': randint(100, 300),         # Number of boosting stages
    'learning_rate': uniform(0.01, 0.3),       # Shrinks contribution of each tree
    'max_depth': randint(3, 10),               # Max depth of individual trees
    'min_samples_split': randint(2, 10),       # Min samples to split a node
    'min_samples_leaf': randint(1, 10),        # Min samples at leaf node
    'subsample': uniform(0.7, 0.3),            # Fraction of samples for fitting each base learner
    'max_features': ['sqrt', 'log2', None]     # Max features for best split
}

# Initialize model
gb = GradientBoostingClassifier(random_state=42)

# Setup RandomizedSearchCV
random_search_gb = RandomizedSearchCV(
    estimator=gb,
    param_distributions=param_dist,
    n_iter=20,                  # Try 20 random combinations
    cv=3,
  scoring='f1',        # or 'accuracy', 'f1'
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# Fit the model (on resampled and scaled training data)
random_search_gb.fit(X_train_scaled, y_train)

# Best model
best_gb = random_search_gb.best_estimator_

# Show best parameters
print("\nâœ… Best Parameters:", random_search_gb.best_params_)
print("ðŸ“ˆ Best Score (CV):", random_search_gb.best_score_)

rf = RandomForestClassifier(n_estimators = 158, min_samples_split= 10, max_depth =None, random_state=42, class_weight='balanced',max_features='sqrt',min_samples_leaf=1) # You can adjust n_estimators
rf.fit(X_train_scaled, y_train)

from sklearn.linear_model import LogisticRegression
lg = LogisticRegression(penalty='l2', C=np.float64(1.1462107403425035) ,random_state=42,class_weight='balanced', max_iter=200,solver='saga')
lg.fit(X_train_scaled, y_train)

from sklearn.svm import SVC

svm = SVC(kernel= 'rbf', gamma=0.01, C=100, random_state=42, probability=True)
svm.fit(X_train_scaled, y_train)

from xgboost import XGBClassifier
xgb =  XGBClassifier(subsample=1.0, n_estimators=100, max_depth=4, learning_rate=0.05,random_state=42,class_weight='balanced',colsample_bytree=0.8)
xgb.fit(X_train_scaled , y_train)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(weights='distance', n_neighbors=3, algorithm='auto',metric='manhattan')
knn.fit(X_train_scaled , y_train)

from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Use parameters from the Decision Tree Randomized Search (cell HpZWbfFZW9mg)
dt = DecisionTreeClassifier(
    criterion='entropy',
    max_depth=10,
    min_samples_leaf=2,
    min_samples_split=3,
    max_features='sqrt',
    class_weight='balanced',
    random_state=42
)
dt.fit(X_train_scaled , y_train)

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Define the base estimator
base_estimator = DecisionTreeClassifier(max_depth=3, random_state=42)

# Initialize AdaBoost with the base estimator
ada = AdaBoostClassifier(
    estimator=base_estimator,
    n_estimators=237,
    learning_rate= np.float64(0.05666566321361543),
    random_state=42
)
ada.fit(X_train_scaled, y_train)

from sklearn.ensemble import GradientBoostingClassifier

# Gradient Boosting Classifier
gb = GradientBoostingClassifier(subsample=np.float64(0.7047898), n_estimators=199, max_depth=5, learning_rate= np.float64(0.29466566117599996),
max_features='sqrt', min_samples_leaf= 8,
 min_samples_split= 3)
gb.fit(X_train_scaled, y_train)

y_pred_rf = rf.predict(X_test_scaled)
y_pred_lg = lg.predict(X_test_scaled)
y_pred_svm = svm.predict(X_test_scaled)
y_pred_xgb = xgb.predict(X_test_scaled)
y_pred_knn = knn.predict(X_test_scaled)
y_pred_dt = dt.predict(X_test_scaled)
y_pred_gb = gb.predict(X_test_scaled)
y_pred_ada = ada.predict(X_test_scaled)

y_pred_lg_new = []
for element in y_pred_lg:
  if element > 0.5:
    y_pred_lg_new.append(1)
  else:
    y_pred_lg_new.append(0)

y_pred_svm_new = []
for element in y_pred_svm:
  if element > 0.5:
    y_pred_svm_new.append(1)
  else:
    y_pred_svm_new.append(0)

y_pred_xgb_new = []
for element in y_pred_xgb:
  if element > 0.5:
    y_pred_xgb_new.append(1)
  else:
    y_pred_xgb_new.append(0)

y_pred_knn_new = []
for element in y_pred_knn:
  if element > 0.5:
    y_pred_knn_new.append(1)
  else:
    y_pred_knn_new.append(0)

y_pred_dt_new = []
for element in y_pred_dt:
  if element > 0.5:
    y_pred_dt_new.append(1)
  else:
    y_pred_dt_new.append(0)

y_pred_gb_new = []
for element in y_pred_gb:
  if element > 0.5:
    y_pred_gb_new.append(1)
  else:
    y_pred_gb_new.append(0)

y_pred_ada_new = []
for element in y_pred_ada:
  if element > 0.5:
    y_pred_ada_new.append(1)
  else:
    y_pred_ada_new.append(0)

from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, jaccard_score

def evaluate_model(y_true, y_pred, model_name):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    jaccard = jaccard_score(y_true, y_pred, average='weighted')

    print(f"--- {model_name} ---")
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1-Score: {f1}")
    print(f"Jaccard Score: {jaccard}")
    print(classification_report(y_true, y_pred))


evaluate_model(y_test, y_pred_rf, "Random Forest")
evaluate_model(y_test, y_pred_lg_new, "Logistic Regression")
evaluate_model(y_test, y_pred_svm_new, "SVM")
evaluate_model(y_test, y_pred_xgb_new, "XGBoost")
evaluate_model(y_test, y_pred_knn_new, "KNN")
evaluate_model(y_test, y_pred_dt_new, "Decision Tree")
evaluate_model(y_test, y_pred_gb_new, "Gradient Boosting")
evaluate_model(y_test, y_pred_ada_new, "AdaBoost")

from sklearn.model_selection import cross_val_score, cross_validate
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, jaccard_score

scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score, zero_division=0),
    'recall': make_scorer(recall_score, zero_division=0),
    'f1': make_scorer(f1_score, zero_division=0),
    'jaccard': make_scorer(jaccard_score, zero_division=0)
}

def print_cross_validation_scores(model, X, y, name):
    scores = cross_validate(model, X, y, cv=3, scoring=scoring)
    print(f"{name}:")
    print(f"Accuracy: {scores['test_accuracy'].mean():.4f} (+/- {scores['test_accuracy'].std():.4f})")
    print(f"Precision: {scores['test_precision'].mean():.4f} (+/- {scores['test_precision'].std():.4f})")
    print(f"Recall: {scores['test_recall'].mean():.4f} (+/- {scores['test_recall'].std():.4f})")
    print(f"F1 Score: {scores['test_f1'].mean():.4f} (+/- {scores['test_f1'].std():.4f})")
    print(f"Jaccard Score: {scores['test_jaccard'].mean():.4f} (+/- {scores['test_jaccard'].std():.4f})")
    print("-" * 20)


print_cross_validation_scores(rf, x_r, y_r, "Random Forest")
print_cross_validation_scores(lg, x_r, y_r, "Logistic Regression")
print_cross_validation_scores(svm, x_r, y_r, "SVM")
print_cross_validation_scores(xgb, x_r, y_r, "XGBoost")
print_cross_validation_scores(knn, x_r, y_r, "KNN")
print_cross_validation_scores(dt, x_r, y_r, "Decision Tree")
print_cross_validation_scores(ada, x_r, y_r, "AdaBoost")
print_cross_validation_scores(gb, x_r, y_r, "Gradient Boosting")

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score

class CustomStackedEnsemble:
    def __init__(self, random_state=42, use_scaler=True):
        self.use_scaler = use_scaler
        self.base_models = {
            'gb': gb,
            'xgb': xgb,
            'knn': knn,
        }
        self.meta_model = svm
        self.trained_base_models = {}
        self.model_performances = {}

        if self.use_scaler:
            self.scaler = scaler

    def fit(self, X, y):
        X = np.array(X)
        y = np.array(y)

        if self.use_scaler:
            X = self.scaler.fit_transform(X)

        kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
        n_samples = X.shape[0]
        n_classes = len(np.unique(y))
        meta_features = np.zeros((n_samples, len(self.base_models) * n_classes))

        for model_idx, (model_name, model) in enumerate(self.base_models.items()):
            model_meta_features = np.zeros((n_samples, n_classes))
            model_cv_metrics = {
                'accuracy': [],
                'precision': [],
                'recall': [],
                'f1': [],
                'jaccard': [],
                'auc': [],  # AUC metric
                'fold_metrics': []  # To store metrics for each fold
            }

            for train_idx, val_idx in kfold.split(X, y):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                model.fit(X_train, y_train)
                fold_meta_features = model.predict_proba(X_val)
                model_meta_features[val_idx] = fold_meta_features

                y_val_pred = model.predict(X_val)

                # Collect CV metrics
                fold_metrics = {
                    'accuracy': accuracy_score(y_val, y_val_pred),
                    'precision': precision_score(y_val, y_val_pred, average='weighted'),
                    'recall': recall_score(y_val, y_val_pred, average='weighted'),
                    'f1': f1_score(y_val, y_val_pred, average='weighted'),
                    'jaccard': jaccard_score(y_val, y_val_pred, average='weighted'),
                    'auc': roc_auc_score(y_val, fold_meta_features[:, 1])  # AUC calculation
                }
                model_cv_metrics['fold_metrics'].append(fold_metrics)

                # Average the metrics for overall results
                model_cv_metrics['accuracy'].append(fold_metrics['accuracy'])
                model_cv_metrics['precision'].append(fold_metrics['precision'])
                model_cv_metrics['recall'].append(fold_metrics['recall'])
                model_cv_metrics['f1'].append(fold_metrics['f1'])
                model_cv_metrics['jaccard'].append(fold_metrics['jaccard'])
                model_cv_metrics['auc'].append(fold_metrics['auc'])  # Store AUC

            start_idx = model_idx * n_classes
            end_idx = start_idx + n_classes
            meta_features[:, start_idx:end_idx] = model_meta_features

            model.fit(X, y)
            self.trained_base_models[model_name] = model

            # Average the cross-validation metrics for the base model
            self.model_performances[model_name] = {
                'overall': {metric: np.mean(scores) for metric, scores in model_cv_metrics.items() if metric != 'fold_metrics'},
                'fold_metrics': model_cv_metrics['fold_metrics']  # Store the fold metrics for later use
            }

        # Train meta model
        self.meta_model.fit(meta_features, y)
        meta_model_metrics = {
            'overall': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'jaccard': [], 'auc': []},
            'fold_metrics': []  # To store metrics for each fold
        }

        for train_idx, val_idx in kfold.split(X, y):
            X_train, X_val = meta_features[train_idx], meta_features[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            self.meta_model.fit(X_train, y_train)
            y_val_pred = self.meta_model.predict(X_val)

            # Collect CV metrics for meta model
            fold_metrics = {
                'accuracy': accuracy_score(y_val, y_val_pred),
                'precision': precision_score(y_val, y_val_pred, average='weighted'),
                'recall': recall_score(y_val, y_val_pred, average='weighted'),
                'f1': f1_score(y_val, y_val_pred, average='weighted'),
                'jaccard': jaccard_score(y_val, y_val_pred, average='weighted'),
                'auc': roc_auc_score(y_val, self.meta_model.predict_proba(X_val)[:, 1])  # AUC calculation
            }
            meta_model_metrics['fold_metrics'].append(fold_metrics)

            # Average the metrics for overall results
            meta_model_metrics['overall']['accuracy'].append(fold_metrics['accuracy'])
            meta_model_metrics['overall']['precision'].append(fold_metrics['precision'])
            meta_model_metrics['overall']['recall'].append(fold_metrics['recall'])
            meta_model_metrics['overall']['f1'].append(fold_metrics['f1'])
            meta_model_metrics['overall']['jaccard'].append(fold_metrics['jaccard'])
            meta_model_metrics['overall']['auc'].append(fold_metrics['auc'])  # Store AUC

        # Average the cross-validation metrics for the meta model
        self.model_performances['meta_model'] = {
            'overall': {metric: np.mean(scores) for metric, scores in meta_model_metrics['overall'].items()},
            'fold_metrics': meta_model_metrics['fold_metrics']  # Store the fold metrics for the meta model
        }

        return self

    def predict(self, X):
        if self.use_scaler:
            X = self.scaler.transform(X)
        meta_features = self._generate_test_meta_features(X)
        return self.meta_model.predict(meta_features)

    def predict_proba(self, X):
        if self.use_scaler:
            X = self.scaler.transform(X)
        meta_features = self._generate_test_meta_features(X)
        return self.meta_model.predict_proba(meta_features)

    def _generate_test_meta_features(self, X):
        X = np.array(X)
        n_samples = X.shape[0]
        n_classes = len(self.meta_model.classes_)
        meta_features = np.zeros((n_samples, len(self.base_models) * n_classes))

        for model_idx, (model_name, model) in enumerate(self.trained_base_models.items()):
            model_predictions = model.predict_proba(X)
            start_idx = model_idx * n_classes
            end_idx = start_idx + n_classes
            meta_features[:, start_idx:end_idx] = model_predictions

        return meta_features

    def get_model_performance(self):
        return self.model_performances

# Example usage:
if __name__ == "__main__":
    # Initialize and train the stacked ensemble
    stack = CustomStackedEnsemble()
    stack.fit(X_train, y_train)

    # Print cross-validation performances
    print("\nCross-Validation Performances:")
    for model, metrics in stack.get_model_performance().items():
        print(f"\n{model} (Overall):")
        for metric_name, metric_value in metrics['overall'].items():
            print(f"{metric_name.capitalize()}: {metric_value:.4f}")

        # Print per-fold metrics for each model
        print(f"\n{model} (Per Fold Metrics):")
        for fold_idx, fold_metrics in enumerate(metrics['fold_metrics']):
            print(f"  Fold {fold_idx + 1}:")
            for metric_name, metric_value in fold_metrics.items():
                print(f"    {metric_name.capitalize()}: {metric_value:.4f}")

    # Make predictions
    y_pred = stack.predict(X_test)
    y_pred_proba = stack.predict_proba(X_test)

    # Calculate and print test performance
    test_accuracy = accuracy_score(y_test, y_pred)
    test_auc = roc_auc_score(y_test, y_pred_proba[:, 1])

    from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score

class CustomStackedEnsemble:
    def __init__(self, random_state=42, use_scaler=True):
        self.use_scaler = use_scaler
        self.base_models = {
            'gb': gb,
            'xgb': xgb,
            'knn': knn,
        }
        self.meta_model = svm
        self.trained_base_models = {}
        self.model_performances = {}

        if self.use_scaler:
            self.scaler = scaler

    def fit(self, X, y):
        X = np.array(X)
        y = np.array(y)

        if self.use_scaler:
            X = self.scaler.fit_transform(X)

        kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
        n_samples = X.shape[0]
        n_classes = len(np.unique(y))
        meta_features = np.zeros((n_samples, len(self.base_models) * n_classes))

        for model_idx, (model_name, model) in enumerate(self.base_models.items()):
            model_meta_features = np.zeros((n_samples, n_classes))
            model_cv_metrics = {
                'accuracy': [],
                'precision': [],
                'recall': [],
                'f1': [],
                'jaccard': [],
                'auc': [],  # AUC metric
                'fold_metrics': []  # To store metrics for each fold
            }

            for train_idx, val_idx in kfold.split(X, y):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                model.fit(X_train, y_train)
                fold_meta_features = model.predict_proba(X_val)
                model_meta_features[val_idx] = fold_meta_features

                y_val_pred = model.predict(X_val)

                # Collect CV metrics
                fold_metrics = {
                    'accuracy': accuracy_score(y_val, y_val_pred),
                    'precision': precision_score(y_val, y_val_pred, average='weighted'),
                    'recall': recall_score(y_val, y_val_pred, average='weighted'),
                    'f1': f1_score(y_val, y_val_pred, average='weighted'),
                    'jaccard': jaccard_score(y_val, y_val_pred, average='weighted'),
                    'auc': roc_auc_score(y_val, fold_meta_features[:, 1])  # AUC calculation
                }
                model_cv_metrics['fold_metrics'].append(fold_metrics)

                # Average the metrics for overall results
                model_cv_metrics['accuracy'].append(fold_metrics['accuracy'])
                model_cv_metrics['precision'].append(fold_metrics['precision'])
                model_cv_metrics['recall'].append(fold_metrics['recall'])
                model_cv_metrics['f1'].append(fold_metrics['f1'])
                model_cv_metrics['jaccard'].append(fold_metrics['jaccard'])
                model_cv_metrics['auc'].append(fold_metrics['auc'])  # Store AUC

            start_idx = model_idx * n_classes
            end_idx = start_idx + n_classes
            meta_features[:, start_idx:end_idx] = model_meta_features

            model.fit(X, y)
            self.trained_base_models[model_name] = model

            # Average the cross-validation metrics for the base model
            self.model_performances[model_name] = {
                'overall': {metric: np.mean(scores) for metric, scores in model_cv_metrics.items() if metric != 'fold_metrics'},
                'fold_metrics': model_cv_metrics['fold_metrics']  # Store the fold metrics for later use
            }

        # Train meta model
        self.meta_model.fit(meta_features, y)
        meta_model_metrics = {
            'overall': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'jaccard': [], 'auc': []},
            'fold_metrics': []  # To store metrics for each fold
        }

        for train_idx, val_idx in kfold.split(X, y):
            X_train, X_val = meta_features[train_idx], meta_features[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            self.meta_model.fit(X_train, y_train)
            y_val_pred = self.meta_model.predict(X_val)

            # Collect CV metrics for meta model
            fold_metrics = {
                'accuracy': accuracy_score(y_val, y_val_pred),
                'precision': precision_score(y_val, y_val_pred, average='weighted'),
                'recall': recall_score(y_val, y_val_pred, average='weighted'),
                'f1': f1_score(y_val, y_val_pred, average='weighted'),
                'jaccard': jaccard_score(y_val, y_val_pred, average='weighted'),
                'auc': roc_auc_score(y_val, self.meta_model.predict_proba(X_val)[:, 1])  # AUC calculation
            }
            meta_model_metrics['fold_metrics'].append(fold_metrics)

            # Average the metrics for overall results
            meta_model_metrics['overall']['accuracy'].append(fold_metrics['accuracy'])
            meta_model_metrics['overall']['precision'].append(fold_metrics['precision'])
            meta_model_metrics['overall']['recall'].append(fold_metrics['recall'])
            meta_model_metrics['overall']['f1'].append(fold_metrics['f1'])
            meta_model_metrics['overall']['jaccard'].append(fold_metrics['jaccard'])
            meta_model_metrics['overall']['auc'].append(fold_metrics['auc'])  # Store AUC

        # Average the cross-validation metrics for the meta model
        self.model_performances['meta_model'] = {
            'overall': {metric: np.mean(scores) for metric, scores in meta_model_metrics['overall'].items()},
            'fold_metrics': meta_model_metrics['fold_metrics']  # Store the fold metrics for the meta model
        }

        return self

    def predict(self, X):
        if self.use_scaler:
            X = self.scaler.transform(X)
        meta_features = self._generate_test_meta_features(X)
        return self.meta_model.predict(meta_features)

    def predict_proba(self, X):
        if self.use_scaler:
            X = self.scaler.transform(X)
        meta_features = self._generate_test_meta_features(X)
        return self.meta_model.predict_proba(meta_features)

    def _generate_test_meta_features(self, X):
        X = np.array(X)
        n_samples = X.shape[0]
        n_classes = len(self.meta_model.classes_)
        meta_features = np.zeros((n_samples, len(self.base_models) * n_classes))

        for model_idx, (model_name, model) in enumerate(self.trained_base_models.items()):
            model_predictions = model.predict_proba(X)
            start_idx = model_idx * n_classes
            end_idx = start_idx + n_classes
            meta_features[:, start_idx:end_idx] = model_predictions

        return meta_features

    def get_model_performance(self):
        return self.model_performances

# Example usage:
if __name__ == "__main__":
    # Initialize and train the stacked ensemble
    stack = CustomStackedEnsemble()
    stack.fit(X_train, y_train)

    # Print cross-validation performances
    print("\nCross-Validation Performances:")
    for model, metrics in stack.get_model_performance().items():
        print(f"\n{model} (Overall):")
        for metric_name, metric_value in metrics['overall'].items():
            print(f"{metric_name.capitalize()}: {metric_value:.4f}")

        # Print per-fold metrics for each model
        print(f"\n{model} (Per Fold Metrics):")
        for fold_idx, fold_metrics in enumerate(metrics['fold_metrics']):
            print(f"  Fold {fold_idx + 1}:")
            for metric_name, metric_value in fold_metrics.items():
                print(f"    {metric_name.capitalize()}: {metric_value:.4f}")

    # Make predictions
    y_pred = stack.predict(X_test)
    y_pred_proba = stack.predict_proba(X_test)

    # Calculate and print test performance
    test_accuracy = accuracy_score(y_test, y_pred)
    test_auc = roc_auc_score(y_test, y_pred_proba[:, 1])

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Generate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot using seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Stacked Model')
plt.show()

import os

save_path = '/content/drive/MyDrive/CervicalCancer/'
os.makedirs(save_path, exist_ok=True)